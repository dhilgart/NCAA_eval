# Project Brief

**Project Name:** NCAA Tournament Prediction Evaluation System
**Date:** February 11, 2026
**Status:** Ready for PRD

## 1. Project Background & Opportunity
The current landscape of NCAA tournament prediction focuses heavily on "bracket" outcomes, often ignoring the probabilistic nature of modern modeling. There is a lack of standardized tooling to evaluate models based on their full probability matrices (`N*(N-1)/2` matchups) rather than just realized tournament paths. This project aims to build a rigorous, scientifically valid, and highly performant evaluation repository that supports both probabilistic metrics and traditional bracket scoring scenarios.

## 2. Goals & Objectives
* **Standardize Input:** Define a strict Python interface for model submission to ensure consistent evaluation across all model types.
* **Rigorous Validation:** Provide "Leave-One-Tournament-Out" and time-series cross-validation to prevent overfitting and look-ahead bias.
* **Multi-Dimensional Evaluation:** Evaluate models not just on "winning," but on calibration, probabilistic accuracy, ranking ability, and game-theory performance.
* **High Performance:** Architecture designed for speed (vectorization, parallelization) to handle expensive cross-validation tasks and simulations.
* **Extensibility:** A plugin-first architecture allowing users to inject custom metrics, scoring systems, and evaluation methods.

## 3. Core Functional Requirements

### A. Model Interface
* **Abstract Base Class:** Models must inherit from a standard class and implement a `predict(year)` method.
* **Output Format:** Strict return of a **Full Probability Matrix** (dictionary or dataframe) covering every possible matchup, not just the bracket path.

### B. Evaluation Engine
* **Probabilistic Metrics (The "Hard Science"):**
    * **Log Loss:** Standard probabilistic penalty metric.
    * **Brier Score:** Mean squared error of probabilities.
    * **ROC-AUC:** Ranking quality metric.
* **Calibration Analysis:**
    * **Metrics:** ECE (Expected Calibration Error).
    * **Visualizations:** Reliability Diagrams (scatter plot of Predicted vs. Actual probability) to identify over/under-confident models.
* **Tournament-Specific Scoring (The "Game Theory"):**
    * **User-Defined Point Schedules:** Configurable scoring (e.g., default $2^{n-1}$, Fibonacci, etc.) for correct picks.
    * **Bracket Conversion:** Functionality to convert probability matrices into discrete brackets (e.g., "Max Likelihood" or "Chalk" conversion) to calculate realized points.
    * **Expected Points:** Calculation of expected point totals based on raw probabilities.

### C. Validation & Workflow
* **Advanced Cross-Validation:** Native `LeaveOneTournamentOut` and time-series splits to ensure scientific validity.
* **Data Pipeline:** A normalization layer to map diverse data sources (Kaggle, KenPom, etc.) to a single canonical team ID system.
* **Performance Optimization:**
    * **Vectorization:** Use `numpy`/`pandas` for all core metric calculations.
    * **Parallel Processing:** Multi-processing support for running Cross-Validation folds.
    * **Caching:** Intelligent caching of expensive model predictions and data fetches.

### D. Extensibility & UX
* **Plugin Architecture:** Allow users to register custom metrics, scoring functions, and weighting schemes without modifying core code.
* **Visualization Suite:** Auto-generation of Reliability Diagrams, Year-by-Year accuracy plots, and Metric Correlation Heatmaps.
* **Reporting:** Clear CLI summary tables and exportable reports (HTML/Markdown).
* **Fail-Fast Debugging:** Deep logging, error traces, and data assertions to help users debug historical crashes.

## 4. Technical Constraints
* **Language:** Python 3.10+.
* **Interface:** Code-based submission (no CSV uploads).
* **Core Libraries:** `numpy`, `pandas`, `scikit-learn`, `joblib` (for parallelization), `matplotlib`/`seaborn` (for viz).

## 5. Success Metrics
* A user can define a custom scoring system, plug it into the config, and backtest a model across 10 years in under 60 seconds.
* The system produces a "Calibration Report" that clearly distinguishes between a model that is "lucky" (high points, bad physics) and one that is "good" (high points, good physics).
