# Story 3.3: Document Findings & Feature Engineering Recommendations

Status: done

<!-- Note: Validation is optional. Run validate-create-story for quality check before dev-story. -->

## Story

As a data scientist,
I want a synthesized document of EDA findings with actionable recommendations,
So that Epic 4 (Feature Engineering) has clear direction on what features to build and what data issues to address.

## Acceptance Criteria

1. **Given** the data quality audit (Story 3.1) and statistical exploration (Story 3.2) are complete, **When** the data scientist reads the findings document, **Then** confirmed data quality issues are listed with specific cleaning recommendations.
2. **And** promising feature engineering approaches are identified with supporting evidence from EDA.
3. **And** signals worth pursuing are ranked by expected predictive value.
4. **And** known limitations and caveats in the data are documented.
5. **And** the document is committed as a project reference for Epic 4 planning.

## Tasks / Subtasks

- [x] Task 1: Read and internalize existing EDA findings (AC: 1–5)
  - [x] 1.1: Read `notebooks/eda/data_quality_findings.md` (Story 3.1 output — 8 recommendations)
  - [x] 1.2: Read `notebooks/eda/statistical_exploration_findings.md` (Story 3.2 output — 5 sections)
  - [x] 1.3: Review `_bmad-output/implementation-artifacts/3-1-data-quality-audit.md` Completion Notes for additional context
  - [x] 1.4: Review `_bmad-output/implementation-artifacts/3-2-statistical-exploration-relationship-analysis.md` Completion Notes for quantitative findings

- [x] Task 2: Create synthesis document `notebooks/eda/eda_findings_synthesis.md` (AC: 1–5)
  - [x] 2.1: Section 1 — Data Quality Issues & Cleaning Recommendations (AC: 1)
  - [x] 2.2: Section 2 — Ranked Feature Engineering Opportunities with Evidence (AC: 2, 3)
  - [x] 2.3: Section 3 — Known Limitations & Caveats (AC: 4)
  - [x] 2.4: Section 4 — Epic 4 Story-Specific Guidance (AC: 5)

- [x] Task 3: Commit document (AC: 5)
  - [x] 3.1: `git add notebooks/eda/eda_findings_synthesis.md`
  - [x] 3.2: Commit: `docs(eda): synthesize EDA findings and feature engineering recommendations (Story 3.3)`

### Review Follow-ups (AI)

- [ ] [AI-Review][LOW] L1: "All games have dates" bullet in Section 3 (Known Limitations) is a positive fact-correction, not a limitation — consider framing it as a note under Section 1 Issue #2 or adding "(Assumption correction)" prefix [`notebooks/eda/eda_findings_synthesis.md`:163]
- [ ] [AI-Review][LOW] L2: PF/TO_rate ordering inconsistency between Section 2 (PF #5 r=-0.1574, then TO_rate #6 r=-0.1424) and Story 4.4 table (TO_rate first, then PF) — both orderings are defensible but the inconsistency may cause confusion [`notebooks/eda/eda_findings_synthesis.md`:84,205]

## Dev Notes

### Story Nature: Pure Documentation — No Code

This is a **documentation synthesis story**. The primary deliverable is a single Markdown file created by reading and synthesizing the outputs from Stories 3.1 and 3.2. **No new code goes into `src/ncaa_eval/`**. No notebooks. No Python scripts. One markdown file.

`mypy --strict`, Ruff, and the vectorization mandate do NOT apply. No `from __future__ import annotations` required.

### Output File

**Create:** `notebooks/eda/eda_findings_synthesis.md`

This file is the Epic 4 planning reference. Keep it alongside the other EDA findings files in `notebooks/eda/`. Epic 4 create-story workflows and dev agents will be directed here for Feature Engineering guidance.

### Source Files to Read

**Do NOT re-run notebooks or re-compute statistics.** Read only:

1. `notebooks/eda/data_quality_findings.md` — generated by `01_data_quality_audit.ipynb` (Story 3.1)
2. `notebooks/eda/statistical_exploration_findings.md` — generated by `02_statistical_exploration.ipynb` (Story 3.2)
3. `_bmad-output/implementation-artifacts/3-1-data-quality-audit.md` — Dev Notes for raw data shapes and confirmed finding details
4. `_bmad-output/implementation-artifacts/3-2-statistical-exploration-relationship-analysis.md` — Completion Notes for exact correlation values

### Required Document Structure

The synthesis document must cover these sections in this order:

---

```
# NCAA_eval: EDA Findings & Feature Engineering Recommendations

Generated: 2026-02-20
Sources: Stories 3.1 (Data Quality Audit) and 3.2 (Statistical Exploration)

## Section 1: Confirmed Data Quality Issues & Cleaning Actions
## Section 2: Ranked Feature Engineering Opportunities
## Section 3: Known Data Limitations & Caveats
## Section 4: Epic 4 Story-by-Story Guidance
```

---

### Section 1 Content: Data Quality Issues & Cleaning Actions

Synthesize from `data_quality_findings.md` Confirmed Issues + Recommendations. Include all of the following (verified from Story 3.1 actuals):

**Issues confirmed:**

| # | Issue | Severity | Cleaning Action for Epic 4 |
|---|---|---|---|
| 1 | `canonical_name = ""` for all 380 teams | HIGH | Build mapping from `MTeamSpellings.csv` (1,177 entries) in Story 4.3 |
| 2 | 2025 season: 4,545 games stored twice (Kaggle + ESPN IDs) | HIGH | Deduplicate by `(w_team_id, l_team_id, day_num)` — prefer ESPN records |
| 3 | Box-score data not in Parquet repo | HIGH | Epic 4 pipeline decision: ingest into `Game` schema (Option A, recommended) or access CSV directly (Option B) |
| 4 | 48 games with `num_ot >= 4` | LOW | Flag but keep — confirmed real outliers, not errors |
| 5 | 109 games with `w_score > 130`; 168 with `margin > 60` | LOW | Keep — legitimate historical data, not errors |
| 6 | 2025: `loc`/`num_ot` discrepancy between Kaggle and ESPN for same game | MEDIUM | ESPN values preferred (API-verified) — resolved by deduplication |

### Section 2 Content: Ranked Feature Engineering Opportunities

Rank by expected predictive value based on evidence from Stories 3.1 and 3.2. Include quantitative backing:

**Tier 1 — High confidence (direct empirical evidence from Story 3.2):**

1. **Field Goal Percentage (FGPct)** — r=0.2269 with tournament advancement; tournament winners average 0.476 vs. losers 0.397 (+0.078 differential — largest single-stat gap). → Story 4.4 (sequential FGPct rolling avg) + Story 4.7 (feature serving)
2. **Field Goals Made (FGM) / Scoring Volume** — r=0.2628 (highest single correlation); Score r=0.2349. → Story 4.4 (rolling scoring avg)
3. **Strength of Schedule (SoS)** — r=0.2970 with tournament advancement (MEDIUM signal, p=3.16e-53). SoS rises monotonically from R64 (mean 0.516) to Champions (mean 0.562). → Stories 4.5 (graph centrality — stronger SoS proxy) and 4.6 (opponent-adjusted efficiency — most rigorous SoS)
4. **Turnover Rate (TO_rate)** — r=-0.1424; tournament winners average 0.147 vs losers 0.155 (-0.008). → Story 4.4 (rolling TO_rate)
5. **Personal Fouls (PF)** — r=-0.1574 (largest single negative correlation). → Story 4.4 (rolling PF avg)
6. **Defensive Rebounds (DR)** — tournament winner differential: +4.5 per game. → Story 4.4

**Tier 2 — Moderate confidence (structural/domain signals):**

7. **Home Court Advantage** — 65.8% win rate home, +2.2 pts vs. neutral. Important for training data normalization. Declining trend (p=0.0006) suggests time-varying feature. → Story 4.4 (loc feature encoding)
8. **Seed Number** — 1v16 rarely upset (1 in history: UMBC 2018); 5v12, 10v7, 11v6 most volatile (35–40% upset). 8v9 ≈ coin flip. → Story 4.3 (MNCAATourneySeeds.csv integration, 2,626 rows 1985–2025)
9. **Conference Strength** — ACC, Big Ten, Big East, SEC, Big 12 dominate tournament wins; conference realignment creates year-over-year discontinuities. → Story 4.3 (MTeamConferences.csv feature)

**Tier 3 — Speculative (requires research spike):**

10. **Graph Centrality (PageRank, betweenness)** — SoS is a direct proxy for network position; graph centrality captures non-linear schedule strength. → Story 4.5 (NetworkX graphs) — depends on 4.1 spike validation
11. **Opponent-Adjusted Efficiency** — Adjusts FGPct, scoring for opponent quality (KenPom-style). More granular than raw SoS. → Story 4.6 (linear algebra solver)
12. **Massey Ordinal Rankings** — 100+ ranking systems available via `MMasseyOrdinals.csv`. Top systems by season coverage: AP, DOL, COL, MOR, POM (23 seasons each). → Story 4.1 spike (research which systems add signal beyond raw box scores)

### Section 3 Content: Known Limitations & Caveats

**Must include all of these:**

- **Box-score coverage starts 2003**: `MRegularSeasonDetailedResults.csv` covers 2003–2025 only (118,882 rows). No per-game FGM/FGA/etc. before 2003. Pre-2003 features are limited to compact results (W/L, score, margin, location).
- **Tournament detailed results stop at 2024**: `MNCAATourneyDetailedResults.csv` has 1,382 rows through 2024. 2025 tournament box scores unavailable until tournament completes.
- **2020 COVID year**: No tournament. All feature pipelines must: **include** 2020 in training data (regular season played); **exclude** 2020 from evaluation. Use `is_evaluation_year(season: int) -> bool: return season != 2020`.
- **2025 deduplication required**: 4,545 games stored twice. Any aggregation touching 2025 must deduplicate first. See deduplication pattern in Story 3.2 Dev Notes.
- **canonical_name is empty for all teams**: Team ID-based joins are reliable. Name-based joins require `MTeamSpellings.csv` mapping first (Story 4.3).
- **Conference realignment discontinuities**: Conference abbreviations change across years (Big East split, Pac-12 collapse 2024). Treat each `(season, conf_abbrev)` as a distinct entity.
- **Correlation analysis covers 2003–2024 only**: Tournament outcome correlations require detailed stats (2003+) and completed tournament data (through 2024). Correlation values are not applicable to pre-2003 seasons.
- **Survivorship bias in tournament stats**: Tournament games involve only 64–68 elite teams per year. Tournament-computed statistics are not representative of the full D1 population.

### Section 4 Content: Epic 4 Story-by-Story Guidance

Map findings to each Epic 4 story so the Story 4.1 spike and subsequent stories have specific direction:

**Story 4.1 (Spike — Research Feature Engineering Techniques):**
- Research spike confirmed to be needed — EDA provides directional evidence but not definitive rankings
- Priority questions to answer in spike: (1) Do graph centrality metrics add signal beyond naive SoS? (2) Which Massey Ordinal systems best complement box-score features? (3) What distribution family fits each stat best (Beta for rates, Gamma for counts)?
- Reference: `template-requirements.md` Section on "Epic 4 Normalization Design Requirements" — normalization configurability requirements are already specified

**Story 4.2 (Chronological Data Serving API):**
- All features show temporal trends: home advantage declining (p=0.0006), scoring patterns vary by era
- API must support per-game chronological streaming for sequential features (Story 4.4)
- 2020 handling: `get_chronological_season(2020)` must return regular season data; evaluation harness skips 2020

**Story 4.3 (Canonical Team ID Mapping & Data Cleaning):**
- `MTeamSpellings.csv`: 1,177 spelling variants for 380 teams — primary mapping source
- `MNCAATourneySeeds.csv`: 2,626 rows, 1985–2025. Post-2011: 68 teams (First Four). Seed format: `"W01"` → parse with `r"(\d+)"` and `is_play_in = Seed.str.contains(r"[ab]$")`
- `MTeamConferences.csv`: conference membership per season — realignment means same team may appear under different conf_abbrev across years

**Story 4.4 (Sequential Transformations):**
- Rolling FGPct: most predictive single rolling stat (r=0.2269, tournament differential +0.078)
- Rolling SoS (opponent win rate): MEDIUM signal (r=0.2970); compute as running mean of opponent regular-season win rates up to each game date
- Rolling TO_rate and PF: negative predictors, include in feature set
- Location encoding: `loc` field is `Literal['H', 'A', 'N']` — encode as numeric feature for stateless models
- Window sizes to research in 4.1: last 5, 10, 20 games — no strong EDA evidence for optimal window yet

**Story 4.5 (Graph Builders & Centrality):**
- SoS signal (r=0.2970) is the strongest schedule-based signal; PageRank/centrality may improve on this
- Use `networkx` (already in tech stack — see Architecture Section 3)
- Graph should be directed (W→L edge) with edge weight = margin of victory
- Incremental graph updates needed for walk-forward compatibility

**Story 4.6 (Opponent Adjustments):**
- Box-score coverage (2003–2025): sufficient for efficiency adjustments
- Target stats for adjustment: FGPct (highest raw correlation), Scoring efficiency (FGM normalized by possessions)
- Validate against observed SoS (r=0.2970 baseline) — opponent-adjusted efficiency should exceed this

**Story 4.7 (Stateful Feature Serving):**
- Must combine: sequential stats (4.4), graph features (4.5), opponent-adjusted stats (4.6), seed info (4.3)
- Normalization configurability required: `gender_scope: Literal["separate", "combined"] = "separate"` and `dataset_scope: Literal["regular_season", "tournament", "combined"] = "regular_season"` (see `template-requirements.md` Section 9)

### What NOT To Do

- **Do not** create any Python code or notebooks. This is markdown-only.
- **Do not** add any files to `src/ncaa_eval/`.
- **Do not** re-run EDA or re-compute statistics — read the existing findings files.
- **Do not** add the synthesis doc to mypy scope or Ruff checks.
- **Do not** invent correlations or statistics not found in Story 3.1/3.2 outputs.
- **Do not** place the document in `_bmad-output/planning-artifacts/` — keep in `notebooks/eda/` with the other findings files.

### Project Structure Notes

- New file: `notebooks/eda/eda_findings_synthesis.md`
- No changes to `src/`, `tests/`, `pyproject.toml`, or any existing files
- `notebooks/` is tracked by git (not gitignored) — commit the new file directly
- Commit type: `docs(eda): ...` (same pattern as Stories 3.1 and 3.2)
- Branch: Create `story/3-3-document-findings-feature-engineering-recommendations`

**Alignment with Architecture:**
- Architecture Section 9 (Unified Project Structure): `notebooks/` is consistent with a data science project layout
- Architecture Section 5 (Components): Feature Store maps to Stories 4.3–4.7; the recommendations document directly informs the Feature Store implementation
- Architecture FR5: "Sequential Features, Opponent Adjustments, Graph Representations, Normalization" — all are covered in Section 4 guidance above

### References

- [Source: notebooks/eda/data_quality_findings.md — Confirmed Issues + Recommendations table]
- [Source: notebooks/eda/statistical_exploration_findings.md — All 5 sections, correlation values]
- [Source: _bmad-output/implementation-artifacts/3-1-data-quality-audit.md — Completion Notes, confirmed data shapes]
- [Source: _bmad-output/implementation-artifacts/3-2-statistical-exploration-relationship-analysis.md — Completion Notes, exact r-values, tournament differentials]
- [Source: _bmad-output/planning-artifacts/epics.md#Epic 3, Story 3.3 — Acceptance Criteria]
- [Source: _bmad-output/planning-artifacts/epics.md#Epic 4 — Story 4.1–4.7 for guidance mapping]
- [Source: _bmad-output/planning-artifacts/template-requirements.md#Section 9 — Epic 4 Normalization Design Requirements (gender_scope, dataset_scope)]
- [Source: specs/05-architecture-fullstack.md#Section 3 — Tech Stack (NetworkX, Plotly, vectorization mandate)]
- [Source: specs/05-architecture-fullstack.md#Section 9 — Unified Project Structure]

## Dev Agent Record

### Agent Model Used

Claude Sonnet 4.6

### Debug Log References

None — pure documentation story, no code execution or debugging required.

### Completion Notes List

- **Primary deliverable:** `notebooks/eda/eda_findings_synthesis.md` — 4-section synthesis document covering: (1) 6 confirmed data quality issues with cleaning actions, (2) 13 feature engineering opportunities ranked in 3 tiers by empirical evidence, (3) 9 known data limitations and caveats, (4) story-by-story guidance for all 7 Epic 4 stories
- **Source fidelity:** All correlation values, row counts, and statistics taken verbatim from `data_quality_findings.md` and `statistical_exploration_findings.md` — no values invented or extrapolated
- **Epic 4 readiness:** Document includes 2025 deduplication code snippet, seed parsing pattern, and normalization configurability parameters (`gender_scope`, `dataset_scope`) from `template-requirements.md`
- **No code changes:** Zero modifications to `src/ncaa_eval/`, `tests/`, or `pyproject.toml` — pure markdown synthesis as specified
- **Code review fixes applied:** (1) Added explicit ranking rationale paragraph to Section 2 intro explaining why SoS ranks after direct box-score stats despite having the highest r-value (0.2970); (2) Added clarification to SoS description that "MEDIUM signal" is Story 3.2's effect-size classification, not a quality judgment; (3) Added Rolling SoS row to Story 4.7 Feature Source integration table; (4) Clarified Story 4.4 table header to reference ranking rationale

### File List

- `notebooks/eda/eda_findings_synthesis.md` — Created (synthesis document, primary deliverable)
- `_bmad-output/implementation-artifacts/3-3-document-findings-feature-engineering-recommendations.md` — Updated (tasks, status, Dev Agent Record)
- `_bmad-output/implementation-artifacts/sprint-status.yaml` — Updated (`3-3-...` → `review`)

### Change Log

- 2026-02-20: Story 3.3 implemented — EDA findings synthesis document created as `notebooks/eda/eda_findings_synthesis.md` (Agent: Claude Sonnet 4.6)
- 2026-02-20: Code review fixes applied — added ranking rationale, SoS signal clarification, SoS to Story 4.7 table, Story 4.4 table header fix (Agent: Claude Sonnet 4.6)
